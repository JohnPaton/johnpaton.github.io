
<!DOCTYPE html>
<html lang="english">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="https://johnpaton.net/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://johnpaton.net/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="https://johnpaton.net/theme/font-awesome/css/font-awesome.min.css">

    <link href="https://johnpaton.net//styles/custom.css" rel="stylesheet">

    <link href="https://johnpaton.net/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="John Paton Atom">

    <link href="https://johnpaton.net/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="John Paton RSS">

    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#3aa500">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#3aa500">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Microsoft EDGE -->
    <meta name="msapplication-TileColor" content="#3aa500">

<meta name="author" content="John Paton" />
<meta name="description" content="One of the handy features that makes (Py)Spark more flexible than database tools like Hive even for just transforming tabular data is the ease of creating User Defined Functions (UDFs). However, one thing that still remains a little annoying is that you have to separately define a function and declare it as a UDF. With four lines of code you can clean those definitions right up." />
<meta name="keywords" content="spark, python, data, snippets">

<meta property="og:site_name" content="John Paton"/>
<meta property="og:title" content="Cleaner Spark UDF definitions with a little decorator"/>
<meta property="og:description" content="One of the handy features that makes (Py)Spark more flexible than database tools like Hive even for just transforming tabular data is the ease of creating User Defined Functions (UDFs). However, one thing that still remains a little annoying is that you have to separately define a function and declare it as a UDF. With four lines of code you can clean those definitions right up."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://johnpaton.net/posts/clean-spark-udfs/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-11-16 19:48:11-01:00"/>
<!--<meta property="article:modified_time" content=""/> -->
<meta property="article:author" content="https://johnpaton.net/author/john-paton.html">
<meta property="article:section" content="posts"/>
<meta property="article:tag" content="spark"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="data"/>
<meta property="article:tag" content="snippets"/>
<meta property="og:image" content="https://johnpaton.net//images/headshot_wide.jpg">

  <title>John Paton &ndash; Cleaner Spark UDF definitions with a little decorator</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://johnpaton.net">
        <img src="/images/headshot.png" alt="John Paton" title="John Paton">
      </a>
      <h1><a href="https://johnpaton.net/about">John Paton</a></h1>

<p>data scientist<b style="color:#3aa500;font-size:20px;word-spacing:0px"> | </b>consultant</p>
      <nav>
        <ul class="list">

          <li><a href="/">blog home</a></li>
          <li><a href="/tags">tags</a></li>
          <li><a href="/about">about me</a></li>
          <li><a href="/writing-talks">writing & talks</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-github" href="http://www.github.com/johnpaton" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-twitter" href="http://www.twitter.com/jd_paton" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-linkedin" href="http://linkedin.com/in/john-paton" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-file-pdf-o" href="/static/johnpaton-cv.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i></a></li>
        <li><a class="sc-rss" href="/feeds/all.atom.xml" target="_blank"><i class="fa fa-rss"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="clean-spark-udfs">Cleaner Spark UDF definitions with a little decorator</h1>
    <p>
          Posted on Thu 16 November 2017


        &#8226; 3 min read
    </p>
  </header>


  <div>
    <p><em>Update:</em> It turns out the functionality described here is actually standard, and I just recreated an existing feature! Embarrassing. This is why you always read the docs! Thanks to Enrico Rotundo for pointing this out. Nonetheless, knowing how to define your own decorators is useful if you want to e.g. propagate the docstring using <code>functools.wraps</code>, so I'll leave this here for further exploration.</p>
<p>Alternate title: <em>Clean up your Spark jobs with this one weird trick! Apache will hate you!</em></p>
<p>One of the handy features that makes (Py)Spark more flexible than database tools like Hive even for just transforming tabular data is the ease of creating User Defined Functions (UDFs). Although this is <a href="https://community.hortonworks.com/articles/72414/how-to-create-a-custom-udf-for-hive-using-python.html">also possible in Hive directly</a>, the ability to define and call UDFs directly in the Python code of your job makes life a lot easier and provides context to what you're doing. However, one thing that still remains a little annoying is separately defining a Python function and then having to declare it as a Spark UDF.</p>
<p>Consider the trivial example of incrementing all the values in a Spark DataFrame column by 1. We begin by writing the function, and then make a "UDF-ified" version that we can actually use in Spark.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">as</span> <span class="nn">f</span>

<span class="c1"># define the function</span>
<span class="k">def</span> <span class="nf">increment</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># make the udf version</span>
<span class="n">increment_udf</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="n">increment</span><span class="p">)</span>
</pre></div>


<p>By registering our function as a UDF, we tell Spark that this function should be applied to every value in whatever DataFrame column it is applied to, and Spark takes care of distributing the execution across the cluster when we submit our job. We can now use our newly declared <code>increment_udf</code> to increment all the values in a column:</p>
<div class="highlight"><pre><span></span><span class="c1"># increment column &#39;col&#39; by 1</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;col_plus_1&#39;</span><span class="p">,</span> <span class="n">increment_udf</span><span class="p">(</span><span class="s1">&#39;col&#39;</span><span class="p">))</span>
</pre></div>


<p>"Hold up," you say, "you're making it unnecessarily difficult for yourself. Just use the <code>f.udf</code> as a <a href="https://www.python.org/dev/peps/pep-0318/">decorator</a>!" That is indeed an attractive option. The code then condenses to:</p>
<div class="highlight"><pre><span></span><span class="nd">@f.udf</span>
<span class="k">def</span> <span class="nf">increment</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;col_plus_1&#39;</span><span class="p">,</span> <span class="n">increment</span><span class="p">(</span><span class="s1">&#39;col&#39;</span><span class="p">))</span>
</pre></div>


<p>This is a lot better looking, but it comes at the cost of flexibility. The function <code>f.udf</code> optionally takes as a second argument the type of the UDF's output (in terms of the <code>pyspark.sql.types</code> <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types">types</a>). Spark will by default convert UDF outputs to strings, which can be a hassle, especially for complex data types (like arrays), or when the precision is important (float vs. double). To avoid this stringy fate, we have to return to our old pattern:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.types</span> <span class="kn">as</span> <span class="nn">t</span>

<span class="c1"># define the function</span>
<span class="k">def</span> <span class="nf">increment</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># make a typed udf version</span>
<span class="n">increment_udf</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="n">increment</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">IntegerType</span><span class="p">())</span>
</pre></div>


<p>To get back to our nice, clean decorator syntax, we can write a tiny but useful function to generate a UDF decorator that will cast the output to the appropriate type:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">typed_udf</span><span class="p">(</span><span class="n">return_type</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_typed_udf_wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">return_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_typed_udf_wrapper</span>
</pre></div>


<p>The function <code>typed_udf</code> returns a new UDF decorator with the specified return type. We can think of it as a decorator that accepts an argument. For a more in depth overview of this pattern and decorators in general, see <a href="https://www.thecodeship.com/patterns/guide-to-python-function-decorators/">this blog post from The Code Ship</a>. </p>
<p>Now we once again have the nice, clean version of the code, with the added legibility bonus of the UDF's return type being visible right beside its definition:</p>
<div class="highlight"><pre><span></span><span class="nd">@typed_udf</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">IntegerType</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">increment</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;col_plus_1&#39;</span><span class="p">,</span> <span class="n">increment</span><span class="p">(</span><span class="s1">&#39;col&#39;</span><span class="p">))</span>
</pre></div>


<p>Using a decorator instead of making two versions of every function isn't really necessary in this simple example, but if you are defining 20 UDFs your namespace will get awfully cluttered and it'll become harder to track what's going on. With four lines of code you can bring sanity back to your function naming scheme.</p>
<p>Finally, the code for DIY decorators is notoriously difficult to read, so if you're going to copy-paste, here is the snippet with a docstring (and a shameless plug):</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">as</span> <span class="nn">f</span>

<span class="k">def</span> <span class="nf">typed_udf</span><span class="p">(</span><span class="n">return_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make a UDF decorator with the given return type.</span>

<span class="sd">    Example usage:</span>

<span class="sd">    &gt;&gt;&gt; @typed_udf(t.IntegerType())</span>
<span class="sd">    ... def increment(x):</span>
<span class="sd">    ...     return x + 1</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; df = df.withColumn(&#39;col_plus_1&#39;, increment(&#39;col&#39;))</span>

<span class="sd">    See http://johnpaton.net/posts/clean-spark-udfs for more detail.</span>

<span class="sd">    Args:</span>
<span class="sd">        return_type (pyspark.sql.types type): the type that will be</span>
<span class="sd">            output by the function being decorated</span>

<span class="sd">    Returns:</span>
<span class="sd">        function: Typed UDF decorator</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_typed_udf_wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">return_type</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_typed_udf_wrapper</span>
</pre></div>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://johnpaton.net/tag/spark.html">spark</a>
      <a href="https://johnpaton.net/tag/python.html">python</a>
      <a href="https://johnpaton.net/tag/data.html">data</a>
      <a href="https://johnpaton.net/tag/snippets.html">snippets</a>
    </p>
  </div>



    <div class="addthis_relatedposts_inline">


<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'johnpatonblog';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
        Please enable JavaScript to view comments.

</noscript>
<!-- End Disqus -->
</article>

    <footer>
<p>&copy; John Paton 2017</p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - Theme <a href="https://github.com/JohnPaton/flex-mod" target="_blank">modified</a> from <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>'s <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a>
</p>    </footer>
  </main>

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92349567-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->



<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " John Paton ",
  "url" : "https://johnpaton.net",
  "image": "/images/headshot.png",
  "description": "John's deep musings and cheap tricks for Python, data science, machine learning, and more."
}
</script>

</body>
</html>